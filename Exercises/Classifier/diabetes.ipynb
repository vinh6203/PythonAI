{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 230,
      "metadata": {
        "id": "20hg8mEVeBCa"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import Perceptron, PassiveAggressiveClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 231,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y57jdkAHfViZ",
        "outputId": "1ff2ad16-babd-44b9-ca02-304dddd2fdcc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(768, 9)\n",
            "[[  6.    148.     72.    ...   0.627  50.      1.   ]\n",
            " [  1.     85.     66.    ...   0.351  31.      0.   ]\n",
            " [  8.    183.     64.    ...   0.672  32.      1.   ]\n",
            " ...\n",
            " [  5.    121.     72.    ...   0.245  30.      0.   ]\n",
            " [  1.    126.     60.    ...   0.349  47.      1.   ]\n",
            " [  1.     93.     70.    ...   0.315  23.      0.   ]]\n"
          ]
        }
      ],
      "source": [
        "data = pd.read_csv(\"drive/MyDrive/diabetes_data/diabetes.csv\")\n",
        "print(data.shape)\n",
        "data = data.values\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 232,
      "metadata": {
        "id": "8_Owa0bbgGbZ"
      },
      "outputs": [],
      "source": [
        "X = data[:, :-1]  # feature data\n",
        "y = data[:, -1:]  # label\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
        "\n",
        "y_train = y_train[:, 0]\n",
        "y_test = y_test[:, 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 250,
      "metadata": {
        "id": "0nG6GttjiQLf"
      },
      "outputs": [],
      "source": [
        "# model selection (Neural Network)\n",
        "classifier = [#Perceptron(),\n",
        "             #PassiveAggressiveClassifier(),\n",
        "             MLPClassifier(max_iter=5000,\n",
        "                           hidden_layer_sizes=(25,),\n",
        "                           alpha=0.01,\n",
        "                           activation=\"identity\",\n",
        "                           solver=\"sgd\",\n",
        "                           verbose=1),\n",
        "             #DecisionTreeClassifier(),\n",
        "             ]\n",
        "\n",
        "model = classifier[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 251,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4BLqoMxjE4c",
        "outputId": "fa48837d-3a2d-4231-9a35-870aca9684ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.70811782\n",
            "Iteration 2, loss = 0.70496964\n",
            "Iteration 3, loss = 0.70024980\n",
            "Iteration 4, loss = 0.69456439\n",
            "Iteration 5, loss = 0.68798500\n",
            "Iteration 6, loss = 0.68138024\n",
            "Iteration 7, loss = 0.67435231\n",
            "Iteration 8, loss = 0.66781812\n",
            "Iteration 9, loss = 0.66091895\n",
            "Iteration 10, loss = 0.65421367\n",
            "Iteration 11, loss = 0.64787297\n",
            "Iteration 12, loss = 0.64189296\n",
            "Iteration 13, loss = 0.63598097\n",
            "Iteration 14, loss = 0.63049326\n",
            "Iteration 15, loss = 0.62489337\n",
            "Iteration 16, loss = 0.61983592\n",
            "Iteration 17, loss = 0.61499191\n",
            "Iteration 18, loss = 0.61044793\n",
            "Iteration 19, loss = 0.60613380\n",
            "Iteration 20, loss = 0.60199587\n",
            "Iteration 21, loss = 0.59793800\n",
            "Iteration 22, loss = 0.59395594\n",
            "Iteration 23, loss = 0.59036217\n",
            "Iteration 24, loss = 0.58686822\n",
            "Iteration 25, loss = 0.58364674\n",
            "Iteration 26, loss = 0.58040460\n",
            "Iteration 27, loss = 0.57731450\n",
            "Iteration 28, loss = 0.57436244\n",
            "Iteration 29, loss = 0.57144036\n",
            "Iteration 30, loss = 0.56889429\n",
            "Iteration 31, loss = 0.56613677\n",
            "Iteration 32, loss = 0.56363854\n",
            "Iteration 33, loss = 0.56118355\n",
            "Iteration 34, loss = 0.55894098\n",
            "Iteration 35, loss = 0.55662785\n",
            "Iteration 36, loss = 0.55453293\n",
            "Iteration 37, loss = 0.55230731\n",
            "Iteration 38, loss = 0.55035266\n",
            "Iteration 39, loss = 0.54844441\n",
            "Iteration 40, loss = 0.54650993\n",
            "Iteration 41, loss = 0.54464920\n",
            "Iteration 42, loss = 0.54289074\n",
            "Iteration 43, loss = 0.54108287\n",
            "Iteration 44, loss = 0.53949988\n",
            "Iteration 45, loss = 0.53780607\n",
            "Iteration 46, loss = 0.53629001\n",
            "Iteration 47, loss = 0.53477051\n",
            "Iteration 48, loss = 0.53329567\n",
            "Iteration 49, loss = 0.53179907\n",
            "Iteration 50, loss = 0.53042570\n",
            "Iteration 51, loss = 0.52905513\n",
            "Iteration 52, loss = 0.52772633\n",
            "Iteration 53, loss = 0.52641223\n",
            "Iteration 54, loss = 0.52521672\n",
            "Iteration 55, loss = 0.52394015\n",
            "Iteration 56, loss = 0.52277411\n",
            "Iteration 57, loss = 0.52161541\n",
            "Iteration 58, loss = 0.52042142\n",
            "Iteration 59, loss = 0.51939876\n",
            "Iteration 60, loss = 0.51836091\n",
            "Iteration 61, loss = 0.51718754\n",
            "Iteration 62, loss = 0.51626434\n",
            "Iteration 63, loss = 0.51520404\n",
            "Iteration 64, loss = 0.51429551\n",
            "Iteration 65, loss = 0.51337659\n",
            "Iteration 66, loss = 0.51238638\n",
            "Iteration 67, loss = 0.51152652\n",
            "Iteration 68, loss = 0.51062949\n",
            "Iteration 69, loss = 0.50981621\n",
            "Iteration 70, loss = 0.50895618\n",
            "Iteration 71, loss = 0.50811592\n",
            "Iteration 72, loss = 0.50733310\n",
            "Iteration 73, loss = 0.50664427\n",
            "Iteration 74, loss = 0.50586775\n",
            "Iteration 75, loss = 0.50508486\n",
            "Iteration 76, loss = 0.50431538\n",
            "Iteration 77, loss = 0.50366541\n",
            "Iteration 78, loss = 0.50297267\n",
            "Iteration 79, loss = 0.50229567\n",
            "Iteration 80, loss = 0.50163231\n",
            "Iteration 81, loss = 0.50098945\n",
            "Iteration 82, loss = 0.50036116\n",
            "Iteration 83, loss = 0.49981772\n",
            "Iteration 84, loss = 0.49916288\n",
            "Iteration 85, loss = 0.49858123\n",
            "Iteration 86, loss = 0.49806847\n",
            "Iteration 87, loss = 0.49749266\n",
            "Iteration 88, loss = 0.49695000\n",
            "Iteration 89, loss = 0.49642199\n",
            "Iteration 90, loss = 0.49583210\n",
            "Iteration 91, loss = 0.49536745\n",
            "Iteration 92, loss = 0.49485601\n",
            "Iteration 93, loss = 0.49443872\n",
            "Iteration 94, loss = 0.49395063\n",
            "Iteration 95, loss = 0.49347874\n",
            "Iteration 96, loss = 0.49303194\n",
            "Iteration 97, loss = 0.49256237\n",
            "Iteration 98, loss = 0.49215338\n",
            "Iteration 99, loss = 0.49171351\n",
            "Iteration 100, loss = 0.49133565\n",
            "Iteration 101, loss = 0.49090200\n",
            "Iteration 102, loss = 0.49050446\n",
            "Iteration 103, loss = 0.49018956\n",
            "Iteration 104, loss = 0.48973911\n",
            "Iteration 105, loss = 0.48944076\n",
            "Iteration 106, loss = 0.48903846\n",
            "Iteration 107, loss = 0.48867018\n",
            "Iteration 108, loss = 0.48837741\n",
            "Iteration 109, loss = 0.48799450\n",
            "Iteration 110, loss = 0.48768254\n",
            "Iteration 111, loss = 0.48737070\n",
            "Iteration 112, loss = 0.48707965\n",
            "Iteration 113, loss = 0.48673439\n",
            "Iteration 114, loss = 0.48648471\n",
            "Iteration 115, loss = 0.48617324\n",
            "Iteration 116, loss = 0.48587734\n",
            "Iteration 117, loss = 0.48559968\n",
            "Iteration 118, loss = 0.48534060\n",
            "Iteration 119, loss = 0.48505675\n",
            "Iteration 120, loss = 0.48480701\n",
            "Iteration 121, loss = 0.48455408\n",
            "Iteration 122, loss = 0.48429974\n",
            "Iteration 123, loss = 0.48406916\n",
            "Iteration 124, loss = 0.48380746\n",
            "Iteration 125, loss = 0.48355426\n",
            "Iteration 126, loss = 0.48335381\n",
            "Iteration 127, loss = 0.48313094\n",
            "Iteration 128, loss = 0.48291010\n",
            "Iteration 129, loss = 0.48269821\n",
            "Iteration 130, loss = 0.48251253\n",
            "Iteration 131, loss = 0.48228010\n",
            "Iteration 132, loss = 0.48208887\n",
            "Iteration 133, loss = 0.48187879\n",
            "Iteration 134, loss = 0.48170755\n",
            "Iteration 135, loss = 0.48152821\n",
            "Iteration 136, loss = 0.48131801\n",
            "Iteration 137, loss = 0.48114939\n",
            "Iteration 138, loss = 0.48099421\n",
            "Iteration 139, loss = 0.48080670\n",
            "Iteration 140, loss = 0.48063941\n",
            "Iteration 141, loss = 0.48048332\n",
            "Iteration 142, loss = 0.48030850\n",
            "Iteration 143, loss = 0.48016032\n",
            "Iteration 144, loss = 0.48002997\n",
            "Iteration 145, loss = 0.47985037\n",
            "Iteration 146, loss = 0.47973284\n",
            "Iteration 147, loss = 0.47956994\n",
            "Iteration 148, loss = 0.47943289\n",
            "Iteration 149, loss = 0.47932159\n",
            "Iteration 150, loss = 0.47915995\n",
            "Iteration 151, loss = 0.47902497\n",
            "Iteration 152, loss = 0.47890348\n",
            "Iteration 153, loss = 0.47880964\n",
            "Iteration 154, loss = 0.47867932\n",
            "Iteration 155, loss = 0.47854473\n",
            "Iteration 156, loss = 0.47841283\n",
            "Iteration 157, loss = 0.47832325\n",
            "Iteration 158, loss = 0.47817902\n",
            "Iteration 159, loss = 0.47808088\n",
            "Iteration 160, loss = 0.47799054\n",
            "Iteration 161, loss = 0.47789630\n",
            "Iteration 162, loss = 0.47777452\n",
            "Iteration 163, loss = 0.47767692\n",
            "Iteration 164, loss = 0.47756244\n",
            "Iteration 165, loss = 0.47748478\n",
            "Iteration 166, loss = 0.47737250\n",
            "Iteration 167, loss = 0.47727686\n",
            "Iteration 168, loss = 0.47719382\n",
            "Iteration 169, loss = 0.47708915\n",
            "Iteration 170, loss = 0.47702047\n",
            "Iteration 171, loss = 0.47692263\n",
            "Iteration 172, loss = 0.47685709\n",
            "Iteration 173, loss = 0.47676128\n",
            "Iteration 174, loss = 0.47667791\n",
            "Iteration 175, loss = 0.47657883\n",
            "Iteration 176, loss = 0.47654658\n",
            "Iteration 177, loss = 0.47643502\n",
            "Iteration 178, loss = 0.47635611\n",
            "Iteration 179, loss = 0.47629774\n",
            "Iteration 180, loss = 0.47622208\n",
            "Iteration 181, loss = 0.47615582\n",
            "Iteration 182, loss = 0.47608203\n",
            "Iteration 183, loss = 0.47601905\n",
            "Iteration 184, loss = 0.47594031\n",
            "Iteration 185, loss = 0.47586779\n",
            "Iteration 186, loss = 0.47583346\n",
            "Iteration 187, loss = 0.47576346\n",
            "Iteration 188, loss = 0.47571197\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Precision: 0.7547169811320755\n",
            "Recall   : 0.625\n",
            "F1 score : 0.6837606837606838\n"
          ]
        }
      ],
      "source": [
        "# Normalized data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "# Model training and performance evaluation\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Precision:\", precision_score(y_test, y_pred))\n",
        "print(\"Recall   :\", recall_score(y_test, y_pred))\n",
        "print(\"F1 score :\", f1_score(y_test, y_pred))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
